package reverted_index;
import java.io.BufferedReader;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.util.Comparator;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Scanner;
import java.util.TreeMap;

import connect_database.Selector;
import utils.Stemmer;

public class Index {
	int N = 0; //document quantity
	HashMap<String, HashMap<Integer, Double>> index = new HashMap<>(); // Entire reverted index for all documents
	TreeMap<Integer, Double> Elength = new TreeMap<>();     // Euclidean length for each document
	TreeMap<Integer, Integer> d_length = new TreeMap<>();    // Document length for each document
	double ave_length; // Average length for all document
	public static HashSet<String> stopWords; // Stop words loaded from data/common-english-words.txt
	
	/**
	 * Tokensize a string
	 * para: str - the string to be tokenized
	 * return: an array of tokens
	 */
	public static String[] tokenize(String str) {
		String[] strr = str.split("[\\s\\.,:;\\\"\\'\\|/~!@#$%^&*()+={}-]+");
		return strr;
	}
	
	/**
	 * Get the set of stop words
	 * return: a set of stop words
	 */
	public static HashSet<String> getStopWords() {
		if (stopWords == null || stopWords.size() == 0) {
			stopWords = new HashSet<>();
			try {
				BufferedReader reader = new BufferedReader(new FileReader("data/common-english-words.txt"));
				String str = reader.readLine();
				String strr[] = str.split(",");
				for (int i = 0; i < strr.length; i++) {
					stopWords.add(strr[i]);
				}
				reader.close();
			} catch(Exception e) {
				e.printStackTrace();
			}
		}
		return stopWords;
	}
	
	/** 
	 * Load TF-IDF for all the terms in each document
	 * from the result files generated by MapReduce
	 * @param filepath
	 */
	public void loadTF_IDF(String filepath) {
		try {
			Scanner sc = new Scanner(new FileReader(filepath));
			while (sc.hasNextLine()) {
				String line = sc.nextLine();
				String[] strs = line.split("	");
				String term = strs[0];
				String[] docNum_value = strs[1].split("&");
				int docNum = Integer.parseInt(docNum_value[0]);
				double tf_idf = Double.parseDouble(docNum_value[1]);
				if (this.index.containsKey(term)) {
					this.index.get(term).put(docNum, tf_idf);
				} else {
					HashMap<Integer, Double> temp = new HashMap<>();
					temp.put(docNum, tf_idf);
					this.index.put(term, temp);
				}
			}
			sc.close();
		} catch (FileNotFoundException e) {
			e.printStackTrace();
		}
	}
	
	/**
	 * Count Elength for each document
	 */
	private void countElength() {
		// Get the sum of square of each element in a vector for all documents
		HashMap<Integer, Double> docs;
		double tf_idf;
		Iterator<HashMap<Integer, Double>> iter1 = this.index.values().iterator();
		// Iterate the index for all tf_idf
		// to calculate Elength^2
		while (iter1.hasNext()) {
			docs = iter1.next();
			for (int docID : docs.keySet()) {
				tf_idf = docs.get(docID);
				if (this.Elength.containsKey(docID)) {
					this.Elength.replace(docID, this.Elength.get(docID) + tf_idf * tf_idf);
				}
				else {
					this.Elength.put(docID, tf_idf * tf_idf);
				}
			}
		}
		// Get square root as Elength
		for (int id : this.Elength.keySet()) {
			Elength.replace(id, Math.sqrt(this.Elength.get(id)));
		}
	}
	
	/**
	 * Count average length of
	 * all the documents
	 */
	private void countAvg_length() {
		// Get the average length for all documents
		HashMap<Integer, Double> docs;
		Iterator<HashMap<Integer, Double>> iter1 = this.index.values().iterator();
		//iterate the index for all tf_idf to compute the length for all the documents
		while (iter1.hasNext()) {
			docs = iter1.next();
			for (int docID : docs.keySet()) {
				if (this.d_length.containsKey(docID)) {
					this.d_length.replace(docID, this.d_length.get(docID) + 1);
				}
				else {
					this.d_length.put(docID, 1);
				}
			}
		}
		
		double sum = 0;
		for (int dLength : this.d_length.values()) {
			sum += dLength;
		}
		this.ave_length = sum / this.d_length.size();
	}
	
	/**
	 * Combine all the count-method
	 */
	public void countAll() {
		this.countElength();
		this.countAvg_length();
	}
	
	/**
	 * Make an index for the test article
	 * @param query - the query to get an index
	 * @return an index of the query
	 */
	public TreeMap<String, Double> getQuery(String query) {
		// Result:
		// key as term
		// value first as term frequency and then as counted weight according to term frequency
		TreeMap<String, Double> result = new TreeMap<>();
		HashSet<String> stopWords = Index.getStopWords();
		Stemmer stemmer;
		
		String str[] = Index.tokenize(query);
		for (int i = 0; i < str.length; i++) {
			String s = str[i].toLowerCase();
			//skip stop words
			if (stopWords.contains(s))
				continue;
			
			// stem the s
			stemmer = new Stemmer();
			stemmer.add(s.toCharArray(), s.length());
			stemmer.stem();
			s = stemmer.toString();
			if (!result.containsKey(s)) { // term not in the index
				result.put(s, 1 / (double)s.length());
			} else { //term already in the index, then term frequency plus one
				result.put(s, result.get(s) + 1 / (double)s.length());
			}
		}
		
		double tf;
		// Iterate the index to count TF weight
		for (String term : result.keySet()) {
			tf = result.get(term);
			result.replace(term, (tf + Math.log(0.5)));
		}
		return result;
	}
	
	/*
	 * Get top k matched documents
	 * para: k - the number of documents to get
	 * 		 toMatch - the TreeMap got by method "getQuery"
	 * return: the top-k-matched documents and corresponding scores
	 */
	public TreeMap<Double, Integer> getTopK(int k, TreeMap<String, Double> queryToMatch) {
		TreeMap<Integer, Double> scores = new TreeMap<>();  //score for each document
		TreeMap<Double, Integer> maxScores = new TreeMap<>(new Comparator<Double>() {
			@Override
			public int compare(Double o1, Double o2) {
				return o2.compareTo(o1);
			}
		}); // Sorter to find max scores
		
		//null String to search
		if (queryToMatch.containsKey("") && queryToMatch.size() == 1) {
			int count = k;
			Iterator<Integer> iterE = this.Elength.keySet().iterator();
			while (iterE.hasNext() && count-- > 0) {
				maxScores.put((double) count, iterE.next());
			}
			return maxScores;
		}
		
		
		/**
		 * Below is to count all the scores
		 */
		HashMap<Integer, Double> docList;
		double product;
		// Iterate the "queryToMatch" index to count inner product
		for (String term : queryToMatch.keySet()) {
			if (this.index.get(term) != null) {
				double queryTermWeight = queryToMatch.get(term);
				docList = this.index.get(term);
				for (int docID : docList.keySet()) {
					product = docList.get(docID) * queryTermWeight;
					if (scores.containsKey(docID)) {
						scores.replace(docID, scores.get(docID) + product);
					}
					else {
						scores.put(docID, product);
					}
				}
			}
		}
		
		// Count the Euclidean length of "queryToMatch" index
		double sum = 0;
		double weight;
		for (String term : queryToMatch.keySet()) {
			weight = queryToMatch.get(term);
			sum += weight * weight;
		}
		sum = Math.sqrt(sum);
		
		// Count the cosine similarity for each document
		double score;
		for (int id : scores.keySet()) {
			score = scores.get(id);
			scores.replace(id, scores.get(id) / (this.Elength.get(id) * sum));
		}
		
		// Time punishment
		TreeMap<Integer, Integer> yearGap = Selector.getYearGap(scores.keySet());
		double a = 0.2; //time attenuation parameter
		for (int id : scores.keySet()) {
			score = scores.get(id);
			score *= (1 / (1 + a * yearGap.get(id)));
			scores.replace(id, score);
		}
		
		//Below is to sort and find out the max k scores
		for (int id : scores.keySet()) {
			score = scores.get(id);
			maxScores.put(score, id);
		}
		//Put k max scores into TreeMap sorted descending
		TreeMap<Double, Integer> topK = new TreeMap<>(new Comparator<Double>() {
			@Override
			public int compare(Double o1, Double o2) {
				return o2.compareTo(o1);
			}
		});
		int docID;
		int count = 0;
		for (double scoreKey : maxScores.keySet()) {
			docID = maxScores.get(scoreKey);
			if (Double.compare(scoreKey, 0) != 0 && !Double.isNaN(scoreKey)) {
				topK.put(scoreKey, docID);
				count++;
			}
			if (count >= k)
				break;
		}
		return topK;
	}
}
